{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Model: a model of the probability of a sequence of words <br>\n",
    "Ex. \"The quick brown fox jumps over the lazy dog.\" <br>\n",
    "A language model allows me to calculate: $P(\"\\text{The quick brown fox jumps over the lazy dog.}\")$\n",
    "\n",
    "Bigram model: $P(w_t | w_{t-1})$. <br>\n",
    "This is simple counting: $P(brown | quick) = \\frac{count(quick \\rightarrow brown)}{count(quick)}$\n",
    "\n",
    "Need a set of documents for sentences to train the model.\n",
    "\n",
    "Bayes Rule: $P(\"A B C\") = P(C|A B)\\times P(B|A)\\times P(A)$, or called chain rule of probability.\n",
    "\n",
    "Problem: if a sentence or word does not exist in the corpus but make sense, the model would still produce 0 probability.\n",
    "\n",
    "**Add-one Smoothing**: instead of maximum-likelihood counting, add a small number to each count. <br>\n",
    "Note: V := vocabulary size = number of distinct words <br>\n",
    "$$P_{smooth}(B|A) = \\frac{count(A \\rightarrow B) + 1}{count(A) + V}$$\n",
    "This still assigns a small probability even though it doesn't appear in corpus.\n",
    "\n",
    "**The Markov Assumption**: what I see now depends *only* on what I saw in the previous step.\n",
    "$$P(w_t|W_{t-1},\\cdots,w_1) = P(w_t|w_{t-1})$$\n",
    "\n",
    "First Markov Order Assumption: $P(E|A,B,C,D) = P(E|D)\\times P(D|C)\\times P(C|B)\\times P(B|A)\\times P(A)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-16T18:26:07.604658Z",
     "start_time": "2018-07-16T18:26:06.551380Z"
    }
   },
   "outputs": [],
   "source": [
    "# import\n",
    "from __future__ import print_function, division\n",
    "from future.utils import iteritems\n",
    "from builtins import range, input\n",
    "import operator\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-16T18:26:08.902528Z",
     "start_time": "2018-07-16T18:26:08.890364Z"
    }
   },
   "outputs": [],
   "source": [
    "KEEP_WORDS = set([\n",
    "  'king', 'man', 'queen', 'woman',\n",
    "  'italy', 'rome', 'france', 'paris',\n",
    "  'london', 'britain', 'england',\n",
    "])\n",
    "\n",
    "\n",
    "def get_sentences():\n",
    "    # returns 57340 of the Brown corpus\n",
    "    # each sentence is represented as a list of individual string tokens\n",
    "    return brown.sents()\n",
    "\n",
    "\n",
    "def get_sentences_with_word2idx():\n",
    "    sentences = get_sentences()\n",
    "    indexed_sentences = []\n",
    "\n",
    "    i = 2\n",
    "    word2idx = {'START': 0, 'END': 1}\n",
    "    for sentence in sentences:\n",
    "        indexed_sentence = []\n",
    "        for token in sentence:\n",
    "            token = token.lower()\n",
    "            if token not in word2idx:\n",
    "                word2idx[token] = i\n",
    "                i += 1\n",
    "            indexed_sentence.append(word2idx[token])\n",
    "        indexed_sentences.append(indexed_sentence)\n",
    "\n",
    "    print(\"Vocab size:\", i)\n",
    "    return indexed_sentences, word2idx\n",
    "\n",
    "\n",
    "def get_sentences_with_word2idx_limit_vocab(n_vocab=2000, keep_words=KEEP_WORDS):\n",
    "    sentences = get_sentences()\n",
    "    indexed_sentences = []\n",
    "\n",
    "    i = 2\n",
    "    word2idx = {'START': 0, 'END': 1}\n",
    "    idx2word = ['START', 'END']\n",
    "\n",
    "    word_idx_count = {\n",
    "        0: float('inf'),\n",
    "        1: float('inf'),\n",
    "    }\n",
    "\n",
    "    for sentence in sentences:\n",
    "        indexed_sentence = []\n",
    "        for token in sentence:\n",
    "            token = token.lower()\n",
    "            if token not in word2idx:\n",
    "                idx2word.append(token)\n",
    "                word2idx[token] = i\n",
    "                i += 1\n",
    "\n",
    "            # keep track of counts for later sorting\n",
    "            idx = word2idx[token]\n",
    "            word_idx_count[idx] = word_idx_count.get(idx, 0) + 1\n",
    "\n",
    "            indexed_sentence.append(idx)\n",
    "        indexed_sentences.append(indexed_sentence)\n",
    "\n",
    "    # restrict vocab size\n",
    "\n",
    "    # set all the words I want to keep to infinity\n",
    "    # so that they are included when I pick the most\n",
    "    # common words\n",
    "    for word in keep_words:\n",
    "        word_idx_count[word2idx[word]] = float('inf')\n",
    "\n",
    "    sorted_word_idx_count = sorted(word_idx_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    word2idx_small = {}\n",
    "    new_idx = 0\n",
    "    idx_new_idx_map = {}\n",
    "    for idx, count in sorted_word_idx_count[:n_vocab]:\n",
    "        word = idx2word[idx]\n",
    "        print(word, count)\n",
    "        word2idx_small[word] = new_idx\n",
    "        idx_new_idx_map[idx] = new_idx\n",
    "        new_idx += 1\n",
    "    # let 'unknown' be the last token\n",
    "    word2idx_small['UNKNOWN'] = new_idx \n",
    "    unknown = new_idx\n",
    "\n",
    "    assert('START' in word2idx_small)\n",
    "    assert('END' in word2idx_small)\n",
    "    for word in keep_words:\n",
    "        assert(word in word2idx_small)\n",
    "\n",
    "    # map old idx to new idx\n",
    "    sentences_small = []\n",
    "    for sentence in indexed_sentences:\n",
    "        if len(sentence) > 1:\n",
    "            new_sentence = [idx_new_idx_map[idx] if idx in idx_new_idx_map else unknown for idx in sentence]\n",
    "            sentences_small.append(new_sentence)\n",
    "\n",
    "    return sentences_small, word2idx_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-16T18:26:13.121330Z",
     "start_time": "2018-07-16T18:26:09.584117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 49817\n"
     ]
    }
   ],
   "source": [
    "indexed_sentences, word2idx = get_sentences_with_word2idx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-16T18:26:14.734776Z",
     "start_time": "2018-07-16T18:26:14.732125Z"
    }
   },
   "outputs": [],
   "source": [
    "V = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-16T18:26:15.076152Z",
     "start_time": "2018-07-16T18:26:15.073507Z"
    }
   },
   "outputs": [],
   "source": [
    "start_idx = word2idx[\"START\"]\n",
    "end_idx = word2idx[\"END\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-16T18:26:15.422018Z",
     "start_time": "2018-07-16T18:26:15.417592Z"
    }
   },
   "outputs": [],
   "source": [
    "# function for bigram probs\n",
    "def get_bigram_probs(sentences, V, start_idx, end_idx, smoothing=1):\n",
    "    # structure ofbigram prob matrix:\n",
    "    #   (last_word_idx, current_word_idx) -> probabiliry\n",
    "    # we will use add-1 smoothing\n",
    "    \n",
    "    bigram_probs = np.ones((V,V)) * smoothing\n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            if i == 0:\n",
    "                bigram_probs[start_idx, sentence[i]] += 1\n",
    "            else:\n",
    "                bigram_probs[sentence[i-1], sentence[i]] += 1\n",
    "            \n",
    "            if i == len(sentence) -1:\n",
    "                bigram_probs[sentence[i], end_idx] += 1\n",
    "        \n",
    "    # normalize it\n",
    "    bigram_probs /= bigram_probs.sum(axis=1, keepdims=True)\n",
    "    return bigram_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-16T18:27:54.579703Z",
     "start_time": "2018-07-16T18:26:16.739543Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_probs = get_bigram_probs(indexed_sentences, V, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-16T18:34:52.011635Z",
     "start_time": "2018-07-16T18:34:52.007601Z"
    }
   },
   "outputs": [],
   "source": [
    "# score function\n",
    "def get_score(sentence):\n",
    "    score = 0\n",
    "    for i in range(len(sentence)):\n",
    "        if i == 0:\n",
    "            score += np.log(bigram_probs[[start_idx], sentence[i]])\n",
    "        else:\n",
    "            score += np.log(bigram_probs[sentence[i-1], sentence[i]])\n",
    "        \n",
    "    score += np.log(bigram_probs[sentence[-1], end_idx])\n",
    "    return score / (len(sentence) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-16T18:38:46.538293Z",
     "start_time": "2018-07-16T18:38:46.522950Z"
    }
   },
   "outputs": [],
   "source": [
    "idx2word = dict((v,k) for k, v in iteritems(word2idx))\n",
    "\n",
    "# function to convert index back to real sentence\n",
    "def get_words(sentence):\n",
    "    return \" \".join(idx2word[x] for x in sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-16T18:32:02.921518Z",
     "start_time": "2018-07-16T18:32:02.917522Z"
    }
   },
   "outputs": [],
   "source": [
    "# sampling\n",
    "start_idx = word2idx[\"START\"]\n",
    "end_idx = word2idx[\"END\"]\n",
    "\n",
    "sample_probs = np.ones(V)\n",
    "sample_probs[start_idx] = 0\n",
    "sample_probs[end_idx] = 0\n",
    "sample_probs /= sample_probs.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-16T18:38:47.562290Z",
     "start_time": "2018-07-16T18:38:47.558254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[138, 1457, 6652, 23447, 41, 25798, 137, 25831, 44, 2, 3269, 22766, 1455, 1455]\n",
      "is your sporting firearms and ammunition department primed for the expanding horizons ? ?\n",
      "[-8.24288217]\n"
     ]
    }
   ],
   "source": [
    "# a real sentence\n",
    "real_idx = np.random.choice(len(indexed_sentences))\n",
    "real = indexed_sentences[real_idx]\n",
    "print(real)\n",
    "print(get_words(real))\n",
    "print(get_score(real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-16T18:38:50.978928Z",
     "start_time": "2018-07-16T18:38:50.973653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12965 37763  7607 23441 32957 12083 42921 28736 48658 11557 10964 48990\n",
      " 46639  3819]\n",
      "nationalized belt-driven racket $350 seurat understandably blueberry nonobservant cutest reacting greet fire-colored beaded $457,000\n",
      "[-10.8672143]\n"
     ]
    }
   ],
   "source": [
    "# a fake sentence\n",
    "fake = np.random.choice(V, size=len(real), p=sample_probs)\n",
    "print(fake)\n",
    "print(get_words(fake))\n",
    "print(get_score(fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-16T18:33:16.799420Z",
     "start_time": "2018-07-16T18:33:16.795619Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
